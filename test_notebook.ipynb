{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.model import NerFormer\n",
    "from network.feature_network import FeatureNet\n",
    "from positional_embedding import HarmonicEmbedding\n",
    "from ray_sampling import RaySampler\n",
    "from co3d_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, IO, Any, List, Tuple, Optional\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = torch.tensor([[-0.3, 1.0, 1.2], [1.6, -1.0, -1.1]])\n",
    "u2 = torch.tensor([[1.0, 1.0, -1.0], [0.7, 0.5, 1.0]])\n",
    "u3 = torch.tensor([[0.5, -0.8, 1.0], [-0.1, 0.3, 0.4]])\n",
    "u4 = torch.tensor([[1.0, 0.1, -0.2], [-0.2, 1.3, -0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    x[x <= 0] = 0\n",
    "    x[x > 0] = 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "tensor([[0.7000],\n",
      "        [0.6000]])\n",
      "tensor([[1.],\n",
      "        [1.]])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[1.0000],\n",
      "        [2.2000]])\n",
      "tensor([[1.],\n",
      "        [1.]])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.7000],\n",
      "        [0.6000]])\n",
      "tensor([[1.],\n",
      "        [1.]])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[0.9000],\n",
      "        [0.7000]])\n",
      "tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1], [0]], dtype=torch.float)\n",
    "x = torch.cat((torch.tensor([[1]]), x), dim=0)\n",
    "print(x)\n",
    "\n",
    "y1 = u1.matmul(x)\n",
    "print(y1)\n",
    "y1 = step(y1)\n",
    "print(y1)\n",
    "\n",
    "y1 = torch.cat((torch.tensor([[1]]), y1), dim=0)\n",
    "print(y1)\n",
    "\n",
    "y2 = u2.matmul(y1)\n",
    "print(y2)\n",
    "y2 = step(y2)\n",
    "print(y2)\n",
    "\n",
    "y2 = torch.cat((torch.tensor([[1]]), y2), dim=0)\n",
    "print(y2)\n",
    "\n",
    "y3 = u3.matmul(y2)\n",
    "print(y3)\n",
    "y3 = step(y3)\n",
    "print(y3)\n",
    "\n",
    "y3 = torch.cat((torch.tensor([[1]]), y3), dim=0)\n",
    "print(y3)\n",
    "\n",
    "y4 = u4.matmul(y3)\n",
    "print(y4)\n",
    "y4 = step(y4)\n",
    "print(y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "tensor([[1.0000],\n",
      "        [0.7000],\n",
      "        [0.6000]])\n",
      "tensor([[1.0000],\n",
      "        [1.1200],\n",
      "        [0.2400]])\n",
      "tensor([[1.0000],\n",
      "        [1.1080],\n",
      "        [0.2160]])\n",
      "tensor([[1.0672],\n",
      "        [0.2544]])\n"
     ]
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "x = torch.tensor([[1], [0]], dtype=torch.float)\n",
    "x = torch.cat((torch.tensor([[1]]), x), dim=0)\n",
    "print(x)\n",
    "\n",
    "y1 = relu(u1.matmul(x))\n",
    "y1 = torch.cat((torch.tensor([[1]]), y1), dim=0)\n",
    "print(y1)\n",
    "\n",
    "y2 = relu(u1.matmul(y1))\n",
    "y2 = torch.cat((torch.tensor([[1]]), y2), dim=0)\n",
    "print(y2)\n",
    "\n",
    "y3 = relu(u1.matmul(y2))\n",
    "y3 = torch.cat((torch.tensor([[1]]), y3), dim=0)\n",
    "print(y3)\n",
    "\n",
    "y4 = relu(u1.matmul(y3))\n",
    "print(y4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u3 = torch.tensor([[0.5, -0.8, 1.0], [-0.1, 0.3, 0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 102 102 102\n",
      "teddybear/38_1655_5016/images/frame000001.jpg\n",
      "teddybear/38_1655_5016/masks/frame000001.png\n",
      "tensor([[-0.9966, -0.0065,  0.0825, -0.0459],\n",
      "        [ 0.0041, -0.9996, -0.0289,  0.2112],\n",
      "        [ 0.0826, -0.0285,  0.9962, -0.9091],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "tensor([[523.4543,   0.0000, 239.5000,   0.0000],\n",
      "        [  0.0000, 294.0334, 179.5000,   0.0000],\n",
      "        [  0.0000,   0.0000,   1.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   1.0000]])\n"
     ]
    }
   ],
   "source": [
    "seq_imgs, seq_masks, seq_c2ws, seq_intrinsics = read_seq_data(\"./test_dataset/38_1655_5016\")\n",
    "\n",
    "print(len(seq_imgs), len(seq_masks), len(seq_c2ws), len(seq_intrinsics))\n",
    "\n",
    "print(seq_imgs[0])\n",
    "print(seq_masks[0])\n",
    "print(seq_c2ws[0])\n",
    "print(seq_intrinsics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teddybear/38_1655_5016/images/frame000002.jpg\n",
      "tensor([[-0.9966, -0.0056,  0.0828, -0.0463],\n",
      "        [ 0.0034, -0.9996, -0.0270,  0.2104],\n",
      "        [ 0.0829, -0.0266,  0.9962, -0.9069],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "tensor([[522.2864,   0.0000, 239.5000,   0.0000],\n",
      "        [  0.0000, 293.3774, 179.5000,   0.0000],\n",
      "        [  0.0000,   0.0000,   1.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(seq_imgs[1])\n",
    "print(seq_c2ws[1])\n",
    "print(seq_intrinsics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# c2w 계산 과정 확인\n",
    "Rotation과 Translation 값을 가지고 [ R | t ] 형태로 만든 것을 w2c로 정의.\n",
    "\n",
    "c2w를 계산하기 위해\n",
    "\n",
    "- w2c를 바로 inverse 시킨 값\n",
    "- R^-1 = R^T, t^-1 = R^T x (-t)  ------> [ R^(-1) | t^(-1) ]\n",
    "\n",
    "두 값이 동일한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_number 0\n",
      "frame_timestamp -1.0\n",
      "image {'path': 'teddybear/38_1655_5016/images/frame000001.jpg', 'size': [479, 359]}\n",
      "depth {'path': 'teddybear/38_1655_5016/depths/frame000001.jpg.geometric.png', 'scale_adjustment': 1.262808918952942, 'mask_path': 'teddybear/38_1655_5016/depth_masks/frame000001.png'}\n",
      "mask {'path': 'teddybear/38_1655_5016/masks/frame000001.png', 'mass': 35984.0}\n",
      "viewpoint {'R': [[-0.9965706467628479, 0.004121924750506878, 0.08264364302158356], [-0.006493818014860153, -0.9995740652084351, -0.028452031314373016], [0.08249115943908691, -0.028891131281852722, 0.9961729049682617]], 'T': [0.20043912529945374, 1.2990046739578247, 6.429634094238281], 'focal_length': [2.1856130105871343, 1.6380690413377479], 'principal_point': [0.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./test_dataset/38_1655_5016/frame_annotations_file.json\", 'r') as f:\n",
    "    j = json.load(f)\n",
    "\n",
    "frame = j[0]\n",
    "\n",
    "for frame_key in frame.keys():\n",
    "    print(frame_key, frame[frame_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2w, k = get_c2w_intrinsic(frame[\"image\"][\"size\"], frame[\"viewpoint\"])\n",
    "\n",
    "r = torch.eye(3)\n",
    "r[:3, :3] = torch.tensor(frame[\"viewpoint\"][\"R\"], dtype=torch.float)\n",
    "\n",
    "t = torch.tensor(frame[\"viewpoint\"][\"T\"], dtype=torch.float).reshape((3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2w : \n",
      " tensor([[-9.9657e-01, -6.4938e-03,  8.2491e-02, -3.2220e-01],\n",
      "        [ 4.1219e-03, -9.9957e-01, -2.8891e-02,  1.4834e+00],\n",
      "        [ 8.2644e-02, -2.8452e-02,  9.9617e-01, -6.3846e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])\n",
      "\n",
      "rotation : \n",
      " tensor([[-0.9966,  0.0041,  0.0826],\n",
      "        [-0.0065, -0.9996, -0.0285],\n",
      "        [ 0.0825, -0.0289,  0.9962]])\n",
      "\n",
      "translation : \n",
      " tensor([[0.2004],\n",
      "        [1.2990],\n",
      "        [6.4296]])\n",
      "\n",
      "intrinsic : \n",
      " tensor([[523.4543,   0.0000, 239.5000,   0.0000],\n",
      "        [  0.0000, 294.0334, 179.5000,   0.0000],\n",
      "        [  0.0000,   0.0000,   1.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(c2w)\n",
    "print(r)\n",
    "print(t)\n",
    "\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse rotation (R^T) : \n",
      " tensor([[-0.9966, -0.0065,  0.0825],\n",
      "        [ 0.0041, -0.9996, -0.0289],\n",
      "        [ 0.0826, -0.0285,  0.9962]])\n",
      "\n",
      "inverse translation (R^T x -t) : \n",
      " tensor([[-0.3222],\n",
      "        [ 1.4834],\n",
      "        [-6.3846]]) torch.Size([3, 1])\n",
      "tensor([[-9.9657e-01, -6.4938e-03,  8.2491e-02, -3.2220e-01],\n",
      "        [ 4.1219e-03, -9.9957e-01, -2.8891e-02,  1.4834e+00],\n",
      "        [ 8.2644e-02, -2.8452e-02,  9.9617e-01, -6.3846e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])\n",
      "c2w ([R^T | t]): \n",
      " tensor([[-9.9657e-01, -6.4938e-03,  8.2491e-02, -3.2220e-01],\n",
      "        [ 4.1219e-03, -9.9957e-01, -2.8891e-02,  1.4834e+00],\n",
      "        [ 8.2644e-02, -2.8452e-02,  9.9617e-01, -6.3846e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "r_T = r.T\n",
    "print(\"inverse rotation (R^T) : \\n\", r_T)\n",
    "\n",
    "t_T = r_T.matmul(-t)\n",
    "print(\"\\ninverse translation (R^T x -t) : \\n\", t_T, t_T.shape)\n",
    "\n",
    "c2w_2 = torch.eye(4, dtype=torch.float)\n",
    "c2w_2[:3, :3] = r_T\n",
    "c2w_2[:3, 3] = t_T.T\n",
    "print(c2w_2)\n",
    "\n",
    "print(\"c2w ([R^T | t]): \\n\", c2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2c (= [R | t]) : \n",
      " tensor([[-9.9657e-01,  4.1219e-03,  8.2644e-02,  2.0044e-01],\n",
      "        [-6.4938e-03, -9.9957e-01, -2.8452e-02,  1.2990e+00],\n",
      "        [ 8.2491e-02, -2.8891e-02,  9.9617e-01,  6.4296e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])\n",
      "\n",
      "c2w (= [R | t]^-1) : \n",
      " tensor([[-9.9657e-01, -6.4938e-03,  8.2491e-02, -3.2220e-01],\n",
      "        [ 4.1219e-03, -9.9957e-01, -2.8891e-02,  1.4834e+00],\n",
      "        [ 8.2644e-02, -2.8452e-02,  9.9617e-01, -6.3846e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "w2c = torch.eye(4, dtype=torch.float)\n",
    "\n",
    "w2c[:3, :3] = r\n",
    "w2c[:3, 3] = t.T\n",
    "\n",
    "print(\"w2c (= [R | t]) : \\n\", w2c)\n",
    "print(\"\\nc2w (= [R | t]^-1) : \\n\", torch.inverse(w2c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_world : \n",
      "tensor([10., 20., 30.,  1.]) \n",
      "\n",
      "P_camera = w2c(P_world) : \n",
      "tensor([ -7.2035, -19.6110,  36.5619,   1.0000]) \n",
      "\n",
      "P_camera convert back to wrold = c2w(P_camera) : \n",
      "tensor([10., 20., 30.,  1.])\n"
     ]
    }
   ],
   "source": [
    "P_w = torch.tensor([10, 20, 30, 1], dtype=torch.float)\n",
    "P_c = w2c.matmul(P_w)\n",
    "P_c_2_w = c2w.matmul(P_c)\n",
    "\n",
    "print(f\"P_world : \\n{P_w} \\n\\nP_camera = w2c(P_world) : \\n{P_c} \\n\\nP_camera convert back to wrold = c2w(P_camera) : \\n{P_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerf_llff_bds = np.load(\"./poses_bounds.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# NeRF llff 데이터셋에서의 bounds 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.04872613e-02  9.98137190e-01 -6.01013048e-02 -3.37871546e+00\n",
      "  3.02400000e+03  9.99658714e-01 -1.19034697e-02 -2.32542981e-02\n",
      " -3.09885180e+00  4.03200000e+03 -2.39263938e-02 -5.98369192e-02\n",
      " -9.97921375e-01  4.17063527e-02  3.32986996e+03  2.94571964e+01\n",
      "  1.14827880e+02]\n",
      "(3, 5, 55)\n",
      "(2, 55)\n",
      "[[ 1.04872613e-02  9.98137190e-01 -6.01013048e-02 -3.37871546e+00\n",
      "   3.02400000e+03]\n",
      " [ 9.99658714e-01 -1.19034697e-02 -2.32542981e-02 -3.09885180e+00\n",
      "   4.03200000e+03]\n",
      " [-2.39263938e-02 -5.98369192e-02 -9.97921375e-01  4.17063527e-02\n",
      "   3.32986996e+03]] \n",
      "\n",
      "[ 29.45719638 114.82787963] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "len(nerf_llff_bds)\n",
    "print(nerf_llff_bds[0])\n",
    "\n",
    "llff_poses = nerf_llff_bds[:, :-2].reshape([-1, 3, 5]).transpose([1, 2, 0])\n",
    "print(llff_poses.shape)\n",
    "llff_bds = nerf_llff_bds[:, -2:].transpose([1, 0])\n",
    "print(llff_bds.shape)\n",
    "print(llff_poses[..., 0], '\\n')\n",
    "print(llff_bds[..., 0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Ray sampling 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈를 1로 설정하여 테스트\n",
    "c2w_ = seq_c2ws[0].unsqueeze(0)\n",
    "intrinsic_ = seq_intrinsics[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지의 크기 지정\n",
    "W = 10\n",
    "H = 20\n",
    "u, v = np.meshgrid(np.arange(W), np.arange(H))\n",
    "\n",
    "# 이미지의 각 row들이 한 줄로 이어붙은 형태로 변환\n",
    "# (H, W) --> (H*W)\n",
    "u = u.reshape(-1).astype(dtype=np.float32)  # + 0.5    # add half pixel\n",
    "v = v.reshape(-1).astype(dtype=np.float32)  # + 0.5\n",
    "\n",
    "pixels = np.stack((u, v, np.ones_like(u)), axis=0)  # [3(x+y+z), H*W]\n",
    "pixels = torch.from_numpy(pixels)\n",
    "batched_pixels = pixels.unsqueeze(0).repeat(1, 1, 1)\n",
    "\n",
    "# 각 픽셀로 향하는 rays의 방향을 구함\n",
    "rays_d = (c2w_[:, :3, :3].bmm(torch.inverse(intrinsic_[:, :3, :3])).bmm(batched_pixels)).transpose(1, 2)\n",
    "rays_d = rays_d.reshape(-1, 3)\n",
    "\n",
    "# 각 픽셀로 향하는 rays의 원점을 구함\n",
    "rays_o = c2w_[:, :3, 3].unsqueeze(1).repeat(1, rays_d.shape[0], 1).reshape(-1, 3)  # B x HW x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rays_d : \n",
      "tensor([[0.5424, 0.5794, 0.9757],\n",
      "        [0.5405, 0.5794, 0.9759],\n",
      "        [0.5386, 0.5795, 0.9760],\n",
      "        [0.5367, 0.5795, 0.9762],\n",
      "        [0.5348, 0.5795, 0.9764],\n",
      "        [0.5329, 0.5795, 0.9765],\n",
      "        [0.5310, 0.5795, 0.9767],\n",
      "        [0.5291, 0.5795, 0.9768],\n",
      "        [0.5272, 0.5795, 0.9770],\n",
      "        [0.5253, 0.5795, 0.9772]]) \n",
      "rays_o : \n",
      "tensor([[-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091],\n",
      "        [-0.0459,  0.2112, -0.9091]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"rays_d : \\n{rays_d[:10]} \\nrays_o : \\n{rays_o[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Positional Embedding 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0998,  0.1987,  0.3894,  0.7174,\n",
      "         0.9996, -0.0584,  0.1165,  0.2315,  0.4504,  0.8043,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.9950,  0.9801,  0.9211,  0.6967, -0.0292, -0.9983,\n",
      "         0.9932,  0.9728,  0.8928,  0.5942])\n",
      "torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "# (x, y, z) ---> 3(x, y, z) * 2(sin, cos) * 10 = 60으로 변환\n",
    "pe = HarmonicEmbedding(10)\n",
    "\n",
    "pe_test = pe(torch.tensor([0, 0, 1]))\n",
    "print(pe_test)\n",
    "print(pe_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import imageio\n",
    "\n",
    "\n",
    "class CO3Ddataset(Dataset):\n",
    "    def __init__(self, args, mode, categories=[], **kwargs):\n",
    "        self.folder_path = os.path.join(args.rootdir, 'CO3D/')\n",
    "        self.rectify_inplane_rotation = args.rectify_inplane_rotation\n",
    "\n",
    "        if mode == 'validation':\n",
    "            mode = 'val'\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "        self.mode = mode  # train / test / val w\n",
    "\n",
    "        self.num_source_views = args.num_source_views\n",
    "\n",
    "        total_category = os.listdir(self.folder_path)\n",
    "\n",
    "        if len(categories) > 0:\n",
    "            if isinstance(categories, str):\n",
    "                categories = [categories]\n",
    "        else:\n",
    "            categories = total_category\n",
    "\n",
    "        print(\"loading {} for {}\".format(categories, mode))\n",
    "        \n",
    "        self.tgt_imgs = []\n",
    "        self.tgt_poses = []\n",
    "        self.tgt_intrinsics = []\n",
    "\n",
    "        for category in categories:\n",
    "            self.category_path = os.path.join(self.folder_path, category)     # \".../CO3D/teddybear\"\n",
    "\n",
    "            rgb_files, c2ws, intrinsics  = read_category_data(self.category_path)\n",
    "            \n",
    "            if self.mode != 'train':\n",
    "                rgb_files = rgb_files[::self.testskip]\n",
    "                intrinsics = intrinsics[::self.testskip]\n",
    "                c2ws = c2ws[::self.testskip]\n",
    "            self.tgt_imgs.extend(rgb_files)\n",
    "            self.tgt_poses.extend(c2ws)\n",
    "            self.tgt_intrinsics.extend(intrinsics)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tgt_imgs)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tgt_img = self.tgt_imgs[idx]\n",
    "        tgt_pose = self.tgt_poses[idx]\n",
    "        tgt_intrinsic = self.tgt_intrinsics[idx]\n",
    "        \n",
    "        # 선택한 데이터가 속한 category, sequence 이름을 가져온다.\n",
    "        category, seq_name = tgt_img.split('/')[:-2]   # \"teddybear\", \"38_1655_5016\"\n",
    "        seq_file_path = os.path.join(self.folder_path, category, seq_name, \"frame_annotations_file.json\")\n",
    "        # 해당 sequence (=오브젝트)의 모든 img, c2w, intrinsic 정보를 읽어온다. --> 소스뷰로 이용\n",
    "        src_imgs, src_c2ws, src_intrinsics = read_seq_data(seq_file_path)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            id_render = int(os.path.basename(tgt_img)[:-4].split('_')[1])\n",
    "            subsample_factor = np.random.choice(np.arange(1, 4), p=[0.3, 0.5, 0.2])\n",
    "        else:\n",
    "            id_render = -1\n",
    "            subsample_factor = 1\n",
    "\n",
    "        rgb = imageio.imread(tgt_img).astype(np.float32) / 255.\n",
    "        rgb = rgb[..., [-1]] * rgb[..., :3] + 1 - rgb[..., [-1]]\n",
    "        img_size = rgb.shape[:2]\n",
    "        camera = np.concatenate((list(img_size), tgt_intrinsic.flatten(),\n",
    "                                 tgt_pose.flatten())).astype(np.float32)\n",
    "\n",
    "        nearest_src_ids = get_nearest_src(tgt_pose,\n",
    "                                                src_c2ws,\n",
    "                                                int(self.num_source_views*subsample_factor),\n",
    "                                                tar_id=id_render,\n",
    "                                                angular_dist_method='vector')\n",
    "        nearest_src_ids = np.random.choice(nearest_src_ids, self.num_source_views, replace=False)\n",
    "\n",
    "        assert id_render not in nearest_src_ids\n",
    "        # occasionally include input image\n",
    "        if np.random.choice([0, 1], p=[0.995, 0.005]) and self.mode == 'train':\n",
    "            nearest_src_ids[np.random.choice(len(nearest_src_ids))] = id_render\n",
    "\n",
    "        src_rgbs = []\n",
    "        src_cameras = []\n",
    "        for id in nearest_src_ids:\n",
    "            src_rgb = imageio.imread(src_imgs[id]).astype(np.float32) / 255.\n",
    "            src_rgb = src_rgb[..., [-1]] * src_rgb[..., :3] + 1 - src_rgb[..., [-1]]\n",
    "            train_pose = src_c2ws[id]\n",
    "            src_intrinsics_ = src_intrinsics[id]\n",
    "            if self.rectify_inplane_rotation:\n",
    "                train_pose, src_rgb = rectify_inplane_rotation(train_pose, tgt_pose, src_rgb)\n",
    "\n",
    "            src_rgbs.append(src_rgb)\n",
    "            img_size = src_rgb.shape[:2]\n",
    "            src_camera = np.concatenate((list(img_size), src_intrinsics_.flatten(),\n",
    "                                              train_pose.flatten())).astype(np.float32)\n",
    "            src_cameras.append(src_camera)\n",
    "\n",
    "        src_rgbs = np.stack(src_rgbs, axis=0)\n",
    "        src_cameras = np.stack(src_cameras, axis=0)\n",
    "\n",
    "        near_depth = 2.\n",
    "        far_depth = 6.\n",
    "\n",
    "        depth_range = torch.tensor([near_depth, far_depth])\n",
    "\n",
    "        return {'rgb': torch.from_numpy(rgb[..., :3]),\n",
    "                'camera': torch.from_numpy(camera),\n",
    "                'rgb_path': tgt_img,\n",
    "                'src_rgbs': torch.from_numpy(src_rgbs[..., :3]),\n",
    "                'src_cameras': torch.from_numpy(src_cameras),\n",
    "                'depth_range': depth_range,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Projector():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def inbound(self, pixel_locations, h, w):\n",
    "        '''\n",
    "        픽셀이 이미지 상의 유효한 위치에 놓여 있는지 확인하는 함수.\n",
    "        param pixel_locations: [..., 2]\n",
    "        param h: height\n",
    "        param w: weight\n",
    "        \n",
    "        return: mask, bool, [...]\n",
    "        '''\n",
    "        return (pixel_locations[..., 0] <= w - 1.) & \\\n",
    "               (pixel_locations[..., 0] >= 0) & \\\n",
    "               (pixel_locations[..., 1] <= h - 1.) &\\\n",
    "               (pixel_locations[..., 1] >= 0)\n",
    "\n",
    "\n",
    "    def normalize(self, pixel_locations, h, w):\n",
    "        resize_factor = torch.tensor([w-1., h-1.]).to(pixel_locations.device)[None, None, :]\n",
    "        normalized_pixel_locations = 2 * pixel_locations / resize_factor - 1.  # [n_views, n_points, 2]\n",
    "        \n",
    "        return normalized_pixel_locations\n",
    "\n",
    "\n",
    "    def compute_projections(self, xyz, train_cameras):\n",
    "        '''\n",
    "        3D 포인트를 train_cameras(소스 카메라)의 이미지 스페이스로 프로젝션하는 함수\n",
    "        param xyz: [..., 3]\n",
    "        param train_cameras: [n_views, 34], 34 = img_size(2) + intrinsics(16) + extrinsics(16)\n",
    "        \n",
    "        return: pixel locations [..., 2], mask [...]\n",
    "        '''\n",
    "        original_shape = xyz.shape[:2]\n",
    "        xyz = xyz.reshape(-1, 3)\n",
    "        num_views = len(train_cameras)\n",
    "\n",
    "        # 타겟 카메라의 intrinsics\n",
    "        train_intrinsics = train_cameras[:, 2:18].reshape(-1, 4, 4)  # [n_views, 4, 4]\n",
    "        # 타겟 카메라의 extrinsics\n",
    "        train_poses = train_cameras[:, -16:].reshape(-1, 4, 4)  # [n_views, 4, 4]\n",
    "        \n",
    "        # (x, y, z) ---> (x, y, z, 1) : Homogeneous Coordinate로 변환.\n",
    "        xyz_h = torch.cat([xyz, torch.ones_like(xyz[..., :1])], dim=-1)  # [n_points, 4]\n",
    "\n",
    "        # P_proj = K x [R|t]_inv x P\n",
    "        projections = train_intrinsics.bmm(torch.inverse(train_poses)) \\\n",
    "            .bmm(xyz_h.t()[None, ...].repeat(num_views, 1, 1))  # [n_views, 4, n_points]\n",
    "\n",
    "        projections = projections.permute(0, 2, 1)  # [n_views, n_points, 4]\n",
    "\n",
    "        # 픽셀 위치 (x, y)만 추출\n",
    "        pixel_locations = projections[..., :2] / torch.clamp(projections[..., 2:3], min=1e-8)  # [n_views, n_points, 2]\n",
    "        pixel_locations = torch.clamp(pixel_locations, min=-1e6, max=1e6)\n",
    "\n",
    "        # 카메라의 뒤로 projection된 포인트는 invalid로 판단.\n",
    "        mask = projections[..., 2] > 0   # a point is invalid if behind the camera\n",
    "        \n",
    "        return pixel_locations.reshape((num_views, ) + original_shape + (2, )), \\\n",
    "               mask.reshape((num_views, ) + original_shape)\n",
    "\n",
    "\n",
    "    def compute_angle(self, xyz, query_camera, train_cameras):\n",
    "        '''\n",
    "        param xyz: [..., 3]\n",
    "        param query_camera: 타겟 카메라 [34, ] : img_size(2) + intrinsics(16) + extrinsics(16)\n",
    "        param train_cameras: 소스 카메라 [n_views, 34]  : img_size(2) + intrinsics(16) + extrinsics(16)\n",
    "        \n",
    "        return: [n_views, ..., 4] : 앞 3개 채널 = query ray와 target ray 사이의 direction 차이를 나타내는 단위 벡터 / 마지막 채널 = 두 방향 벡터의 내적값\n",
    "        '''\n",
    "        original_shape = xyz.shape[:2]\n",
    "        xyz = xyz.reshape(-1, 3)\n",
    "        \n",
    "        train_poses = train_cameras[:, -16:].reshape(-1, 4, 4)  # [n_views, 4, 4]\n",
    "        \n",
    "        num_views = len(train_poses)\n",
    "\n",
    "        # target 카메라의 pose를 소스 개수만큼 복사.\n",
    "        query_pose = query_camera[-16:].reshape(-1, 4, 4).repeat(num_views, 1, 1)  # [n_views, 4, 4]\n",
    "\n",
    "        ray2tar_pose = (query_pose[:, :3, 3].unsqueeze(1) - xyz.unsqueeze(0))\n",
    "        ray2tar_pose /= (torch.norm(ray2tar_pose, dim=-1, keepdim=True) + 1e-6)     # 단위벡터화\n",
    "\n",
    "        ray2train_pose = (train_poses[:, :3, 3].unsqueeze(1) - xyz.unsqueeze(0))\n",
    "        ray2train_pose /= (torch.norm(ray2train_pose, dim=-1, keepdim=True) + 1e-6)     # 단위벡터화\n",
    "\n",
    "        # 두 단위 벡터의 차이값을 계산\n",
    "        ray_diff = ray2tar_pose - ray2train_pose\n",
    "        ray_diff_norm = torch.norm(ray_diff, dim=-1, keepdim=True)  # 단위벡터화\n",
    "        ray_diff_direction = ray_diff / torch.clamp(ray_diff_norm, min=1e-6)\n",
    "\n",
    "        # 두 단위 벡터를 내적\n",
    "        ray_diff_dot = torch.sum(ray2tar_pose * ray2train_pose, dim=-1, keepdim=True)\n",
    "\n",
    "        # [두 ray의 차이를 나타내는 방향 벡터(3채널), 두 ray의 내적값(1채널)]\n",
    "        ray_diff = torch.cat([ray_diff_direction, ray_diff_dot], dim=-1)\n",
    "        ray_diff = ray_diff.reshape((num_views, ) + original_shape + (4, ))\n",
    "\n",
    "        return ray_diff\n",
    "\n",
    "\n",
    "    # 메인으로 실행되는 함수\n",
    "    def compute(self,  xyz, query_camera, train_imgs, train_cameras, featmaps):\n",
    "        '''\n",
    "        :param xyz: [n_rays, n_samples, 3]\n",
    "        :param query_camera: [1, 34], 34 = img_size(2) + intrinsics(16) + extrinsics(16)\n",
    "        :param train_imgs: [1, n_views, h, w, 3] = 소스 이미지 n개\n",
    "        :param train_cameras: [1, n_views, 34] = 소스 카메라 n개 / img_size(2) + intrinsics(16) + extrinsics(16)\n",
    "        :param featmaps: [n_views, d, h, w] = 소스 이미지 n개의 resnet feature map\n",
    "\n",
    "        :return: rgb_feat_sampled: [n_rays, n_samples, 3+n_feat],\n",
    "                 ray_diff: [n_rays, n_samples, 4],\n",
    "                 mask: [n_rays, n_samples, 1]\n",
    "        '''\n",
    "        assert (train_imgs.shape[0] == 1) \\\n",
    "               and (train_cameras.shape[0] == 1) \\\n",
    "               and (query_camera.shape[0] == 1), 'only support batch_size=1 for now'\n",
    "\n",
    "        # 소스 이미지, 소스 카메라의 1이었던 batch 차원을 삭제\n",
    "        train_imgs = train_imgs.squeeze(0)  # [n_views, h, w, 3]\n",
    "        train_cameras = train_cameras.squeeze(0)  # [n_views, 34]\n",
    "        # 타겟 이미지의 1이었던 batch 차원을 삭제\n",
    "        query_camera = query_camera.squeeze(0)  # [34, ]\n",
    "\n",
    "        train_imgs = train_imgs.permute(0, 3, 1, 2)  # 채널 순서를 변경 ---> [n_views, 3, h, w]\n",
    "\n",
    "        h, w = train_cameras[0][:2]\n",
    "\n",
    "        # 쿼리 포인트 (x, y, z)가 각 소스 뷰들로 projection 되는 위치를 계산\n",
    "        pixel_locations, mask_in_front = self.compute_projections(xyz, train_cameras)\n",
    "        normalized_pixel_locations = self.normalize(pixel_locations, h, w)   # [n_views, n_rays, n_samples, 2]\n",
    "\n",
    "        # 각 소스 이미지들로부터 프로젝션 된 위치의 RGB 컬러값을 샘플링\n",
    "        rgbs_sampled = F.grid_sample(train_imgs, normalized_pixel_locations, align_corners=True)\n",
    "        rgb_sampled = rgbs_sampled.permute(2, 3, 0, 1)  # [n_rays, n_samples, n_views, 3]\n",
    "\n",
    "        # 각 소스 이미지의 faeture map들로부터 프로젝션 된 위치의 resnet feature 값을 샘플링\n",
    "        feat_sampled = F.grid_sample(featmaps, normalized_pixel_locations, align_corners=True)\n",
    "        feat_sampled = feat_sampled.permute(2, 3, 0, 1)  # [n_rays, n_samples, n_views, d]\n",
    "        \n",
    "        # [RGB 샘플링 값, resnet feature 샘플링 값]\n",
    "        rgb_feat_sampled = torch.cat([rgb_sampled, feat_sampled], dim=-1)   # [n_rays, n_samples, n_views, d+3]\n",
    "\n",
    "        # mask\n",
    "        inbound = self.inbound(pixel_locations, h, w)\n",
    "        ray_diff = self.compute_angle(xyz, query_camera, train_cameras)\n",
    "        ray_diff = ray_diff.permute(1, 2, 0, 3)\n",
    "        mask = (inbound * mask_in_front).float().permute(1, 2, 0)[..., None]   # [n_rays, n_samples, n_views, 1]\n",
    "\n",
    "        return rgb_feat_sampled, ray_diff, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_sampler = RaySampler(train_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pixels = sample_random_pixel(500, 500, 800, \"center\")\n",
    "print(len(random_pixels), '\\n', random_pixels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rays_o, rays_d = get_rays(img1.height, img1.width, intrinsic.unsqueeze(0), c2w.unsqueeze(0))\n",
    "\n",
    "print(rays_o.shape, rays_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([171961, 3]) torch.Size([171961, 3])\n"
     ]
    }
   ],
   "source": [
    "ray_d = rays_d.clone()\n",
    "ray_o = rays_o.clone()\n",
    "\n",
    "print(ray_d.shape, ray_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_range = torch.tensor([[0.1, 3.0]])\n",
    "N_samples = 20\n",
    "\n",
    "pts, z_vals = sample_along_camera_ray(ray_o, ray_d, depth_range, N_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([171961, 20, 3])\n",
      "torch.Size([171961, 20])\n"
     ]
    }
   ],
   "source": [
    "print(pts.shape)\n",
    "print(z_vals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.4153,  -0.0613,   0.1650],\n",
       "        [ -1.5882,  -0.0688,   0.1851],\n",
       "        [ -3.4004,  -0.1472,   0.3963],\n",
       "        [ -5.2240,  -0.2261,   0.6089],\n",
       "        [ -6.6318,  -0.2871,   0.7730],\n",
       "        [ -7.8613,  -0.3403,   0.9163],\n",
       "        [ -8.7938,  -0.3807,   1.0250],\n",
       "        [-10.1739,  -0.4404,   1.1858],\n",
       "        [-10.7546,  -0.4656,   1.2535],\n",
       "        [-12.8424,  -0.5560,   1.4969],\n",
       "        [-13.9064,  -0.6020,   1.6209],\n",
       "        [-15.5691,  -0.6740,   1.8147],\n",
       "        [-16.1829,  -0.7006,   1.8862],\n",
       "        [-17.7843,  -0.7699,   2.0729],\n",
       "        [-18.9395,  -0.8199,   2.2075],\n",
       "        [-20.2334,  -0.8759,   2.3583],\n",
       "        [-21.3119,  -0.9226,   2.4840],\n",
       "        [-22.5580,  -0.9765,   2.6293],\n",
       "        [-24.7361,  -1.0708,   2.8831],\n",
       "        [-25.7066,  -1.1128,   2.9963]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 400번째 ray에서의 samples\n",
    "pts[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b877b1f7a01ee3f13579b88b14e81a6c392478055cee1e0634b92db773b747d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
