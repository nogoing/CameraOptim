{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.model import NerFormer\n",
    "from network.feature_network import FeatureNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, IO, Any, List, Tuple, Optional\n",
    "import typing\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_annotations_file = \"test_dataset/frame_annotations.jgz\"\n",
    "frame_zipfile = gzip.open(frame_annotations_file, \"rt\", encoding=\"utf8\")\n",
    "\n",
    "frame_dicts = json.load(frame_zipfile)\n",
    "print(type(frame_dicts), len(frame_dicts))\n",
    "\n",
    "sequence_annotations_file = \"test_dataset/test_sequence_annotations.jgz\"\n",
    "seq_zipfile = gzip.open(sequence_annotations_file, \"rt\", encoding=\"utf8\")\n",
    "\n",
    "seq_dicts = json.load(seq_zipfile)\n",
    "print(type(seq_dicts), len(seq_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_dicts[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset image : 1323 ~ 1332\n",
    "data_dicts = frame_dicts[1323:1333]\n",
    "print(data_dicts[0], '\\n\\n', data_dicts[-1])\n",
    "\n",
    "print('\\n\\n', data_dicts[0].keys())\n",
    "print('\\n', data_dicts[0]['viewpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.open(\"test_dataset/frame000001.jpg\")\n",
    "\n",
    "print(img1.size)\n",
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic = torch.eye(4)\n",
    "\n",
    "viewpoint = data_dicts[0]['viewpoint']\n",
    "intrinsic[0][0], intrinsic[1][1] = viewpoint['focal_length'][0], viewpoint['focal_length'][1]\n",
    "intrinsic[0][2], intrinsic[1][2] = viewpoint['principal_point'][0], viewpoint['principal_point'][1]\n",
    "intrinsic[2, 2] = 1\n",
    "\n",
    "print(intrinsic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrinsic = torch.eye(4)\n",
    "\n",
    "rotation = torch.tensor(viewpoint['R'])\n",
    "translation = torch.tensor(viewpoint['T'])\n",
    "\n",
    "print(rotation)\n",
    "print(translation)\n",
    "print(rotation.shape, translation.shape)\n",
    "\n",
    "extrinsic[:3, :3] = rotation\n",
    "extrinsic[:3, 3] = translation\n",
    "\n",
    "print(extrinsic)\n",
    "print(extrinsic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2w = torch.inverse(intrinsic.mul(extrinsic))\n",
    "\n",
    "print(c2w)\n",
    "print(c2w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicEmbedding(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, omega0=0.1):\n",
    "        \"\"\"\n",
    "        Given an input tensor `x` of shape [minibatch, ... , dim],\n",
    "        the harmonic embedding layer converts each feature\n",
    "        in `x` into a series of harmonic features `embedding`\n",
    "        as follows:\n",
    "            embedding[..., i*dim:(i+1)*dim] = [\n",
    "                sin(x[..., i]),\n",
    "                sin(2*x[..., i]),\n",
    "                sin(4*x[..., i]),\n",
    "                ...\n",
    "                sin(2**(self.n_harmonic_functions-1) * x[..., i]),\n",
    "                cos(x[..., i]),\n",
    "                cos(2*x[..., i]),\n",
    "                cos(4*x[..., i]),\n",
    "                ...\n",
    "                cos(2**(self.n_harmonic_functions-1) * x[..., i])\n",
    "            ]\n",
    "            \n",
    "        Note that `x` is also premultiplied by `omega0` before\n",
    "        evaluating the harmonic functions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            'frequencies',\n",
    "            omega0 * (2.0 ** torch.arange(n_harmonic_functions)),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor of shape [..., dim]\n",
    "        Returns:\n",
    "            embedding: a harmonic embedding of `x`\n",
    "                of shape [..., n_harmonic_functions * dim * 2]\n",
    "        \"\"\"\n",
    "        embed = (x[..., None] * self.frequencies).view(*x.shape[:-1], -1)\n",
    "\n",
    "        return torch.cat((embed.sin(), embed.cos()), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = HarmonicEmbedding(10)\n",
    "\n",
    "pe_test = pe(torch.tensor([0, 0, 1]))\n",
    "print(pe_test)\n",
    "print(pe_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 이미지 내에서 랜덤으로 픽셀 샘플링\n",
    "def sample_random_pixel(H, W, N_rand, sample_mode, center_ratio=0.8):\n",
    "    rng = np.random.RandomState(234)\n",
    "\n",
    "    if sample_mode == 'center':\n",
    "        border_H = int(H * (1 - center_ratio) / 2.)\n",
    "        border_W = int(W * (1 - center_ratio) / 2.)\n",
    "\n",
    "        # pixel coordinates\n",
    "        u, v = np.meshgrid(np.arange(border_H, H - border_H),\n",
    "                            np.arange(border_W, W - border_W))\n",
    "        u = u.reshape(-1)\n",
    "        v = v.reshape(-1)\n",
    "\n",
    "        select_inds = rng.choice(u.shape[0], size=(N_rand,), replace=False)\n",
    "        select_inds = v[select_inds] + W * u[select_inds]\n",
    "\n",
    "    elif sample_mode == 'uniform':\n",
    "        # Random from one image\n",
    "        select_inds = rng.choice(H*W, size=(N_rand,), replace=False)\n",
    "    else:\n",
    "        raise Exception(\"unknown sample mode!\")\n",
    "\n",
    "    return select_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pixels = sample_random_pixel(500, 500, 800, \"center\")\n",
    "print(len(random_pixels), '\\n', random_pixels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 이미지의 각 픽셀마다 rays_o, rays_d를 구함\n",
    "def get_rays(H, W, intrinsics, c2w, batch_size=1, render_stride=1):\n",
    "    '''\n",
    "    param H: image height\n",
    "    param W: image width\n",
    "    param intrinsics: 4 by 4 intrinsic matrix\n",
    "    param c2w: 4 by 4 camera to world extrinsic matrix\n",
    "    \n",
    "    return: rays_o, rays_d\n",
    "    '''\n",
    "    # u --> x 인덱싱\n",
    "    # v --> y 인덱싱\n",
    "    u, v = np.meshgrid(np.arange(W)[::render_stride], np.arange(H)[::render_stride])\n",
    "\n",
    "    # 이미지의 각 row들이 이어붙은 형태로 변환\n",
    "    # (H, W) --> (H*W)\n",
    "    u = u.reshape(-1).astype(dtype=np.float32)  # + 0.5    # add half pixel\n",
    "    v = v.reshape(-1).astype(dtype=np.float32)  # + 0.5\n",
    "\n",
    "    pixels = np.stack((u, v, np.ones_like(u)), axis=0)  # [3(x+y+z), H*W]\n",
    "    pixels = torch.from_numpy(pixels)\n",
    "    print(pixels.shape)\n",
    "    batched_pixels = pixels.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    # bmm : batch matrix-matrix product \n",
    "    # [B, N, M] tensor * [B, M, P] tensor >>> [B, N, P]\n",
    "    rays_d = (c2w[:, :3, :3].bmm(torch.inverse(intrinsics[:, :3, :3])).bmm(batched_pixels)).transpose(1, 2)\n",
    "    # rays_d = (c2w[:, :3, :3].bmm(torch.inverse(intrinsics[:, :3, :3])).bmm(pixels)).transpose(1, 2)\n",
    "    rays_d = rays_d.reshape(-1, 3)\n",
    "    rays_o = c2w[:, :3, 3].unsqueeze(1).repeat(1, rays_d.shape[0], 1).reshape(-1, 3)  # B x HW x 3\n",
    "    \n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rays_o, rays_d = get_rays(img1.height, img1.width, intrinsic.unsqueeze(0), c2w.unsqueeze(0))\n",
    "\n",
    "print(rays_o.shape, rays_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_along_camera_ray(ray_o, ray_d, depth_range,\n",
    "                            N_samples,\n",
    "                            inv_uniform=False,\n",
    "                            det=False):\n",
    "    '''\n",
    "    param ray_o: scene coordinate system에서 ray의 원점 : [N_rays, 3]\n",
    "    param ray_d: scene coordinate system에서 ray의 direction vector : [N_rays, 3]\n",
    "    param depth_range: [near_depth, far_depth]\n",
    "    param inv_uniform: if True, uniformly sampling inverse depth\n",
    "    param det: if True, will perform deterministic sampling\n",
    "    \n",
    "    return: tensor of shape [N_rays, N_samples, 3]\n",
    "    '''\n",
    "    # 샘플들은 [near_depth, far_depth] 범위 안에 있어야 함.\n",
    "    # assume the nearest possible depth is at least (min_ratio * depth)\n",
    "    near_depth_value = depth_range[0, 0]\n",
    "    far_depth_value = depth_range[0, 1]\n",
    "    assert near_depth_value > 0 and far_depth_value > 0 and far_depth_value > near_depth_value\n",
    "\n",
    "    near_depth = near_depth_value * torch.ones_like(ray_d[..., 0])\n",
    "    far_depth = far_depth_value * torch.ones_like(ray_d[..., 0])\n",
    "\n",
    "    if inv_uniform:\n",
    "        start = 1. / near_depth     # [N_rays,]\n",
    "        step = (1. / far_depth - start) / (N_samples-1)\n",
    "        inv_z_vals = torch.stack([start+i*step for i in range(N_samples)], dim=1)  # [N_rays, N_samples]\n",
    "        z_vals = 1. / inv_z_vals\n",
    "    else:\n",
    "        start = near_depth\n",
    "        step = (far_depth - near_depth) / (N_samples-1)\n",
    "        z_vals = torch.stack([start+i*step for i in range(N_samples)], dim=1)  # [N_rays, N_samples]\n",
    "\n",
    "    if not det:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[:, 1:] + z_vals[:, :-1])\n",
    "        upper = torch.cat([mids, z_vals[:, -1:]], dim=-1)\n",
    "        lower = torch.cat([z_vals[:, 0:1], mids], dim=-1)\n",
    "        # uniform samples in those intervals\n",
    "        t_rand = torch.rand_like(z_vals)\n",
    "        z_vals = lower + (upper - lower) * t_rand   # [N_rays, N_samples]\n",
    "\n",
    "    ray_d = ray_d.unsqueeze(1).repeat(1, N_samples, 1)  # [N_rays, N_samples, 3]\n",
    "    ray_o = ray_o.unsqueeze(1).repeat(1, N_samples, 1)\n",
    "\n",
    "    # 샘플 포인트 위치를 계산.\n",
    "    # ray의 원점으로부터 ray의 방향으로 z_val만큼 전진한 것.\n",
    "    pts = z_vals.unsqueeze(2) * ray_d + ray_o       # [N_rays, N_samples, 3]\n",
    "    \n",
    "    return pts, z_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([171961, 3]) torch.Size([171961, 3])\n"
     ]
    }
   ],
   "source": [
    "ray_d = rays_d.clone()\n",
    "ray_o = rays_o.clone()\n",
    "\n",
    "print(ray_d.shape, ray_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_range = torch.tensor([[0.1, 3.0]])\n",
    "N_samples = 20\n",
    "\n",
    "pts, z_vals = sample_along_camera_ray(ray_o, ray_d, depth_range, N_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([171961, 20, 3])\n",
      "torch.Size([171961, 20])\n"
     ]
    }
   ],
   "source": [
    "print(pts.shape)\n",
    "print(z_vals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.4153,  -0.0613,   0.1650],\n",
       "        [ -1.5882,  -0.0688,   0.1851],\n",
       "        [ -3.4004,  -0.1472,   0.3963],\n",
       "        [ -5.2240,  -0.2261,   0.6089],\n",
       "        [ -6.6318,  -0.2871,   0.7730],\n",
       "        [ -7.8613,  -0.3403,   0.9163],\n",
       "        [ -8.7938,  -0.3807,   1.0250],\n",
       "        [-10.1739,  -0.4404,   1.1858],\n",
       "        [-10.7546,  -0.4656,   1.2535],\n",
       "        [-12.8424,  -0.5560,   1.4969],\n",
       "        [-13.9064,  -0.6020,   1.6209],\n",
       "        [-15.5691,  -0.6740,   1.8147],\n",
       "        [-16.1829,  -0.7006,   1.8862],\n",
       "        [-17.7843,  -0.7699,   2.0729],\n",
       "        [-18.9395,  -0.8199,   2.2075],\n",
       "        [-20.2334,  -0.8759,   2.3583],\n",
       "        [-21.3119,  -0.9226,   2.4840],\n",
       "        [-22.5580,  -0.9765,   2.6293],\n",
       "        [-24.7361,  -1.0708,   2.8831],\n",
       "        [-25.7066,  -1.1128,   2.9963]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 400번째 ray에서의 samples\n",
    "pts[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071f83251836d5bb3918d2af6501aef1a588d685a567aa45f470f25864dd9495"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
