{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.models.bert.modeling_bert import BertSelfOutput, BertIntermediate, BertOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size_per_step': [80, 40], 'hidden_dropout_prob': 0.2, 'hidden_act': 'gelu', 'num_attention_heads': 8, 'intermediate_size': 80, 'layer_norm_eps': 1e-12, 'initializer_range': 0.02}\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(\"../configs/model/fastformer.yaml\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.hidden_size = config.hidden_size_per_step[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size_per_step': [80, 40], 'hidden_dropout_prob': 0.2, 'hidden_act': 'gelu', 'num_attention_heads': 8, 'intermediate_size': 80, 'layer_norm_eps': 1e-12, 'initializer_range': 0.02, 'hidden_size': 80}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastformer의 핵심인 기존 self-attention의 단순화 버전\n",
    "# Fastformer 논문에서는 d_model = d(논문) = hidden_size(코드)로 어노테이션 되어 있는 듯\n",
    "# 나는 인코더 하나씩만 가져와서 쓰는 거니까 이 클래스까지만 쓰면 될 듯?\n",
    "class FastSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FastSelfAttention, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # d_model은 h로 나누어 떨어져야 함.\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" %\n",
    "                (config.hidden_size, config.num_attention_heads))\n",
    "    \n",
    "        self.attention_head_size = int(config.hidden_size /config.num_attention_heads)      # attention_head_size\n",
    "        self.num_attention_heads = config.num_attention_heads                               # num_attention_heads = h\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size            # all_head_size = d_model = d = hidden_size\n",
    "        self.input_dim = config.hidden_size                                                 # input_dim = d_model = d = hidden_size\n",
    "        \n",
    "        # Query Transformation\n",
    "        # Input >> Query matrix [q_1, q_2, q_3, ..., q_N]\n",
    "        self.query = nn.Linear(self.input_dim, self.all_head_size)      # (..., N, d) >> (..., N, d)\n",
    "        self.query_att = nn.Linear(self.all_head_size, self.num_attention_heads)    # (..., N, d) >> (..., N, num_head)\n",
    "\n",
    "        # Key Transformation\n",
    "        # Input >> Key Matrix [k_1, k_2, k_3, ..., k_N]\n",
    "        self.key = nn.Linear(self.input_dim, self.all_head_size)        # (..., N, d) >> (..., N, d)\n",
    "        self.key_att = nn.Linear(self.all_head_size, self.num_attention_heads)      # (..., N, d) >> (..., N, num_head)\n",
    "        \n",
    "        self.transform = nn.Linear(self.all_head_size, self.all_head_size)      # (..., N, d) >> (..., N, d)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "                \n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n",
    "                                       self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # batch_size, seq_len, num_head * head_dim, batch_size, seq_len\n",
    "        batch_size, seq_len, _ = hidden_states.shape    # (B, N, d)\n",
    "        \n",
    "        # Q: [q_1, q_2, q_3, ..., q_N]\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        # K: [k_1, k_2, k_3, ..., k_N]\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "\n",
    "        # (B, num_head, N)\n",
    "        query_for_score = self.query_att(mixed_query_layer).transpose(1, 2) / self.attention_head_size**0.5     # 각 q벡터에 곱해지는 alpha 계산 후, d**0.5로 나누어 스케일링\n",
    "        # query_for_score += attention_mask       # add attention mask\n",
    "        # (B, num_head, 1, N)\n",
    "        query_weight = self.softmax(query_for_score).unsqueeze(2)       # 계산한 alpha는 query의 weight가 된다.\n",
    "\n",
    "        # batch_size, num_head, seq_len, head_dim\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)  # 여기서부터 해독........\n",
    "\n",
    "        # batch_size, num_head, head_dim, 1\n",
    "        pooled_query = torch.matmul(query_weight, query_layer).transpose(1, 2).view(-1,1,self.num_attention_heads*self.attention_head_size)\n",
    "        pooled_query_repeat= pooled_query.repeat(1, seq_len,1)\n",
    "        # batch_size, num_head, seq_len, head_dim\n",
    "\n",
    "        # batch_size, num_head, seq_len\n",
    "        mixed_query_key_layer=mixed_key_layer* pooled_query_repeat\n",
    "        \n",
    "        query_key_score=(self.key_att(mixed_query_key_layer)/ self.attention_head_size**0.5).transpose(1, 2)\n",
    "        \n",
    "        # query_key_score +=attention_mask      # add attention mask\n",
    "\n",
    "        # batch_size, num_head, 1, seq_len\n",
    "        query_key_weight = self.softmax(query_key_score).unsqueeze(2)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(mixed_query_key_layer)\n",
    "        pooled_key = torch.matmul(query_key_weight, key_layer)\n",
    "\n",
    "        # query = value\n",
    "        weighted_value =(pooled_key * query_layer).transpose(1, 2)\n",
    "        weighted_value = weighted_value.reshape(\n",
    "            weighted_value.size()[:-2] + (self.num_attention_heads * self.attention_head_size,))\n",
    "        weighted_value = self.transform(weighted_value) + mixed_query_layer\n",
    "      \n",
    "        return weighted_value\n",
    "\n",
    "\n",
    "class FastAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FastAttention, self).__init__()\n",
    "\n",
    "        self.self = FastSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        self_output = self.self(input_tensor)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class FastformerLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FastformerLayer, self).__init__()\n",
    "        self.attention = FastAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attention_output = self.attention(hidden_states)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerFormerArchitecture(nn.Module):\n",
    "    def __init__(self, d_z):\n",
    "        super(NerFormerArchitecture, self).__init__()\n",
    "\n",
    "        self.d_z = d_z  # Input feature의 차원\n",
    "\n",
    "        # input: (N_rays(=Batch), N_s, N_src, d_z)\n",
    "        self.linear_1 = nn.Linear(d_z, 80, bias=False)\n",
    "        \n",
    "        # (N_rays, N_s, N_src, 80)\n",
    "        self.TE_1 = nn.Sequential(\n",
    "            TransformerEncoder(along_dim=\"src\", feature_dim=80, num_heads=8),          # Pooling transformer encoder\n",
    "            TransformerEncoder(along_dim=\"sample\", feature_dim=80, num_heads=8)           # Ray transformer encoder\n",
    "        )\n",
    "        self.dim_linear_1 = nn.Linear(80, 40)\n",
    "        # (N_rays, N_s, N_src, 40)\n",
    "        self.TE_2 = nn.Sequential(\n",
    "            TransformerEncoder(along_dim=\"src\", feature_dim=40, num_heads=4),          # Pooling transformer encoder\n",
    "            TransformerEncoder(along_dim=\"sample\", feature_dim=40, num_heads=4)           # Ray transformer encoder\n",
    "        )\n",
    "        self.dim_linear_2 = nn.Linear(40, 20)\n",
    "        # (N_rays, N_s, N_src, 20)\n",
    "\n",
    "        self.weight_layer = nn.Sequential(\n",
    "            nn.Linear(20, 1),\n",
    "            nn.Softmax(dim=-2)      # 특정 sample에서 각 src들에 대한 값의 합이 1이 되도록 차원을 설정\n",
    "        )\n",
    "\n",
    "        # color function head\n",
    "        # Output shape: (N_s, 3)\n",
    "        self.c_head = nn.Sequential(\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 3)\n",
    "        )\n",
    "\n",
    "        # opacity function head\n",
    "        # Output shape: (N_s, 1)\n",
    "        self.f_head = nn.Sequential(\n",
    "            nn.Linear(20, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # input_tensor: (N_rays(=Batch), N_s, N_src, D_z)\n",
    "\n",
    "        x = self.linear_1(input_tensor)     # (N_rays, N_s, N_src, 80)\n",
    "\n",
    "        x = self.TE_1(x)                    # (N_rays, N_s, N_src, 80)\n",
    "        x = self.dim_linear_1(x)              # (N_rays, N_s, N_src, 40)\n",
    "\n",
    "        x = self.TE_2(x)                    # (N_rays, N_s, N_src, 40)\n",
    "        x = self.dim_linear_2(x)              # (N_rays, N_s, N_src, 20)\n",
    "        \n",
    "        # weighted sum along dim 1\n",
    "        weight = self.weight_layer(x)       # (N_rays, N_s, N_src, 1)\n",
    "        per_point_features = torch.sum(weight*x, dim=-2)      # (N_rays, N_s, 20)\n",
    "\n",
    "        # Color function\n",
    "        ray_colors = self.c_head(per_point_features) # (N_rays, N_s, 3)\n",
    "        # Opacity function\n",
    "        ray_densities = self.f_head(per_point_features) # (N_rays, N_s, 1)\n",
    "\n",
    "        return ray_densities, ray_colors\n",
    "\n",
    "\n",
    "# (N_s, N_src, D_z) -> (N_s, N_src, D_z)\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, along_dim, feature_dim, num_heads):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.along_dim = along_dim\n",
    "        # Multi-head attention along dim\n",
    "        # num_heads = 8 (Transformer 논문에서의 세팅)\n",
    "        self.multi_head_att = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads)\n",
    "        self.Q_weights = nn.Linear(feature_dim, feature_dim)\n",
    "        self.K_weights = nn.Linear(feature_dim, feature_dim)\n",
    "        self.V_weights = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(0.1)\n",
    "        self.dropout_2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(feature_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(feature_dim)\n",
    "\n",
    "        self.two_layer_MLP = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # input_tensor = Z\n",
    "\n",
    "        # Multi Head Att = MHA(Z, dim=dim)\n",
    "\n",
    "        # MultiHead(Q,K,V)\n",
    "        # Q: (sequence length, batch, embedding)\n",
    "        # K: (sequence length, batch, embedding)\n",
    "        # V: (sequence length, batch, embedding)\n",
    "\n",
    "        # Pooling transformer enc\n",
    "        if self.along_dim == \"src\":\n",
    "            # 배치로 들어오는 각 샘플들에 대해, N_src개 소스뷰 시퀀스를 입력으로 줌.\n",
    "            # (Seq_len, Batch, Features) = (N_src, N_rays*N_s, D_z)\n",
    "            input_tensor = input_tensor.permute(2, 0, 1, 3)\n",
    "            shape = input_tensor.shape\n",
    "\n",
    "            # Pooling transformer의 Batch에 해당하는\n",
    "            # `N_rays` 차원과 `N_s` 차원을 합쳐준다.\n",
    "            input_tensor = input_tensor.reshape(shape[0], shape[1]*shape[2], shape[3])\n",
    "\n",
    "        # Ray transformer enc\n",
    "        else:\n",
    "            # 배치로 들어오는 각 소스뷰에 대해, N_s개 샘플 시퀀스를 입력으로 줌.\n",
    "            # (Seq_len, Batch, Features) = (N_s, N_rays*N_src, D_Z) \n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3)\n",
    "            shape = input_tensor.shape\n",
    "\n",
    "            # Ray transformer의 Batch에 해당하는\n",
    "            # `N_rays` 차원과 `N_src` 차원을 합쳐준다.\n",
    "            input_tensor = input_tensor.reshape(shape[0], shape[1]*shape[2], shape[3])\n",
    "        \n",
    "        query = self.Q_weights(input_tensor)\n",
    "        key = self.K_weights(input_tensor)\n",
    "        value = self.V_weights(input_tensor)\n",
    "\n",
    "        x, _ = self.multi_head_att(query, key, value)\n",
    "        # Sub-layer MLP\n",
    "        x_skip = self.layer_norm_1(input_tensor + self.dropout_1(x))    # Skip + LayerNorm  = Z'\n",
    "        x = self.two_layer_MLP(x_skip)                                  # Two-Layer MLP = MLP(Z')\n",
    "        x = self.layer_norm_2(x_skip + self.dropout_2(x))               # Skip + LayerNorm = TE^dim(Z)\n",
    "\n",
    "        x = x.reshape(shape[0], shape[1], shape[2], shape[3])   # N_rays 차원을 분리\n",
    "        if self.along_dim == \"src\":\n",
    "            x = x.permute(1, 2, 0, 3)           # 원래 차원 순서인 (N_rays, N_s, N_src, D_z)로 변환\n",
    "        else:\n",
    "            x = x.permute(1, 0, 2, 3)           # 원래 차원 순서인 (N_rays, N_s, N_src, D_z)로 변환\n",
    "\n",
    "        return x        # shape: (N_rays, N_s, N_src, c_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch=ray=1, seq_len=samples=64, sources=3, d_z=160)\n",
    "input_tensor = torch.randn(1, 32, 3, 160).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerformer = NerFormerArchitecture(d_z=160).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 1]) torch.Size([1, 32, 3])\n"
     ]
    }
   ],
   "source": [
    "nerformer_output = nerformer(input_tensor)\n",
    "\n",
    "print(nerformer_output[0].shape, nerformer_output[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastNerFormerArchitecture(nn.Module):\n",
    "    def __init__(self, d_z, config):\n",
    "        super(FastNerFormerArchitecture, self).__init__()\n",
    "\n",
    "        self.d_z = d_z  # Input feature의 차원\n",
    "\n",
    "        # input: (N_rays(=Batch), N_s, N_src, d_z)\n",
    "        self.linear_1 = nn.Linear(d_z, 80, bias=False)\n",
    "        \n",
    "        # (N_rays, N_s, N_src, 80)\n",
    "        config.hidden_size = 80\n",
    "        config.intermediate_size = 80\n",
    "        self.TE_1 = nn.Sequential(\n",
    "            FastformerEncoder(along_dim=\"src\", config=config, step=0),          # Pooling transformer encoder\n",
    "            FastformerEncoder(along_dim=\"sample\", config=config, step=0)           # Ray transformer encoder\n",
    "        )\n",
    "        self.TE_1_2 = nn.Sequential(\n",
    "            FastformerEncoder(along_dim=\"src\", config=config),          # Pooling transformer encoder\n",
    "            FastformerEncoder(along_dim=\"sample\", config=config)           # Ray transformer encoder\n",
    "        )\n",
    "        self.dim_linear_1 = nn.Linear(80, 40)\n",
    "        # (N_rays, N_s, N_src, 40)\n",
    "        config.hidden_size = 40\n",
    "        config.intermediate_size = 40\n",
    "        self.TE_2 = nn.Sequential(\n",
    "            FastformerEncoder(along_dim=\"src\", config=config, step=1),          # Pooling transformer encoder\n",
    "            FastformerEncoder(along_dim=\"sample\", config=config, step=1)           # Ray transformer encoder\n",
    "        )\n",
    "        self.TE_2_2 = nn.Sequential(\n",
    "            FastformerEncoder(along_dim=\"src\", config=config),          # Pooling transformer encoder\n",
    "            FastformerEncoder(along_dim=\"sample\", config=config)           # Ray transformer encoder\n",
    "        )\n",
    "        self.dim_linear_2 = nn.Linear(40, 20)\n",
    "        # (N_rays, N_s, N_src, 20)\n",
    "\n",
    "        self.weight_layer = nn.Sequential(\n",
    "            nn.Linear(20, 1),\n",
    "            nn.Softmax(dim=-2)      # 특정 sample에서 각 src들에 대한 값의 합이 1이 되도록 차원을 설정\n",
    "        )\n",
    "\n",
    "        # color function head\n",
    "        # Output shape: (N_s, 3)\n",
    "        self.c_head = nn.Sequential(\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 3)\n",
    "        )\n",
    "\n",
    "        # opacity function head\n",
    "        # Output shape: (N_s, 1)\n",
    "        self.f_head = nn.Sequential(\n",
    "            nn.Linear(20, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # input_tensor: (N_rays(=Batch), N_s, N_src, D_z)\n",
    "\n",
    "        x = self.linear_1(input_tensor)     # (N_rays, N_s, N_src, 80)\n",
    "\n",
    "        x = self.TE_1(x)                    # (N_rays, N_s, N_src, 80)\n",
    "        x = self.TE_1_2(x)\n",
    "        x = self.dim_linear_1(x)              # (N_rays, N_s, N_src, 40)\n",
    "\n",
    "        x = self.TE_2(x)                    # (N_rays, N_s, N_src, 40)\n",
    "        x = self.TE_2_2(x)\n",
    "        x = self.dim_linear_2(x)              # (N_rays, N_s, N_src, 20)\n",
    "        \n",
    "        # weighted sum along dim 1\n",
    "        weight = self.weight_layer(x)       # (N_rays, N_s, N_src, 1)\n",
    "        per_point_features = torch.sum(weight*x, dim=-2)      # (N_rays, N_s, 20)\n",
    "\n",
    "        # Color function\n",
    "        ray_colors = self.c_head(per_point_features) # (N_rays, N_s, 3)\n",
    "        # Opacity function\n",
    "        ray_densities = self.f_head(per_point_features) # (N_rays, N_s, 1)\n",
    "\n",
    "        return ray_densities, ray_colors\n",
    "\n",
    "\n",
    "# (N_s, N_src, D_z) -> (N_s, N_src, D_z)\n",
    "class FastformerEncoder(nn.Module):\n",
    "    def __init__(self, along_dim, config, step):\n",
    "        super(FastformerEncoder, self).__init__()\n",
    "\n",
    "        self.along_dim = along_dim\n",
    "        # Multi-head attention along dim\n",
    "        # num_heads = 8 (Transformer 논문에서의 세팅)\n",
    "        self.fastformer_layer = FastformerLayer(config=config)\n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Pooling transformer enc\n",
    "        if self.along_dim == \"src\":\n",
    "            # 배치로 들어오는 각 샘플들에 대해, N_src개 소스뷰 시퀀스를 입력으로 줌.\n",
    "            # (Batch, Seq_len, Features) = (N_rays*N_s, N_src, D_z)\n",
    "            shape = input_tensor.shape\n",
    "\n",
    "            # Pooling transformer의 Batch에 해당하는\n",
    "            # `N_rays` 차원과 `N_s` 차원을 합쳐준다.\n",
    "            input_tensor = input_tensor.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "\n",
    "        # Ray transformer enc\n",
    "        else:\n",
    "            # 배치로 들어오는 각 소스뷰에 대해, N_s개 샘플 시퀀스를 입력으로 줌.\n",
    "            # (Batch, Seq_len, Features) = (N_rays*N_src, N_s, D_Z) \n",
    "            input_tensor = input_tensor.permute(0, 2, 1, 3)\n",
    "            shape = input_tensor.shape\n",
    "\n",
    "            # Ray transformer의 Batch에 해당하는\n",
    "            # `N_rays` 차원과 `N_src` 차원을 합쳐준다.\n",
    "            input_tensor = input_tensor.reshape(shape[0]*shape[1], shape[2], shape[3])\n",
    "\n",
    "        x = self.fastformer_layer(input_tensor)\n",
    "\n",
    "        x = x.reshape(shape[0], shape[1], shape[2], shape[3])   # N_rays 차원을 분리\n",
    "        \n",
    "        # 원래 차원 순서인 (N_rays, N_s, N_src, D_z)로 변환\n",
    "        if self.along_dim == \"sample\":\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        return x        # shape: (N_rays, N_s, N_src, c_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastformer_encoder = FastformerEncoder(config).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_tensor = torch.randn(1, 32, 80).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_output_tensor = fastformer_encoder(tmp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 80])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerformer = NerFormerArchitecture(d_z=160).to(\"cuda\")\n",
    "fastnerformer = FastNerFormerArchitecture(d_z=160, config=config).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 3, 160])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 80])\n",
      "torch.Size([3, 32, 80])\n",
      "torch.Size([32, 3, 40])\n",
      "torch.Size([3, 32, 40])\n"
     ]
    }
   ],
   "source": [
    "fastnerformer_output = fastnerformer(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1645460890.3129923"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.randn(800, 32, 3, 160).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nerformer inference :  0.019857168197631836\n",
      "fastnerformer inference :  0.00493621826171875\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "nerformer_output = nerformer(test_tensor)\n",
    "print(\"nerformer inference : \", time.time() - start)\n",
    "\n",
    "start = time.time()\n",
    "fastnerformer_output = fastnerformer(test_tensor)\n",
    "print(\"fastnerformer inference : \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.02 / 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6899858ec4b8e60ed4a600bdd06e23c0a3ce7db43a44c95b45a2fef6e40f9d47"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch3d': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
